%%%%%%%%%%%%%%%%%Epic 7, 8, 9, 10%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Objekte erkennen}

mehrere Epics zusammengefasst 

\subsubsection{Ultraschall}
\label{ultraschall}

Backup-Hinderniserkennung mittels Ultraschallsensor, um Risiko 12 (Objekte werden nicht erkannt) zu mitgieren.

Zur Erhöhung der Betriebssicherheit ist das Fahrzeug mit einer redundanten Hinderniserkennung ausgestattet. Diese erfolgt über einen Ultraschallsensor, der kontinuierlich die Entfernung zu Objekten im unmittelbaren Umfeld misst. Der Sensor ist direkt mit dem Mikrocontroller TinyK22 verbunden.

Die Messwerte werden in Echtzeit ausgewertet. Sobald ein Objekt in einer Distanz von weniger als 10 cm erkannt wird, wird automatisch ein Not-Stopp ausgelöst, um Kollisionen zu vermeiden. Gleichzeitig wird ein definierter Fehlercode generiert und über eine serielle Schnittstelle an den Raspberry Pi übertragen, um den Fehlerstatus zu protokollieren und entsprechende Maßnahmen einleiten zu können.




\subsubsection{Training YOLOv11-Model trainieren}

\paragraph{Einleitung}


Damit der Roboter Pylonen, Knoten und Hindernissen erkennen kann, wurde

 Das Ziel war, das Modell auf einem Raspberry Pi 5 für den Einsatz in einem Roboter zu optimieren. Der Trainingsprozess umfasste drei Iterationen, wobei ich verschiedene Ansätze ausprobierte, um die Modellleistung zu verbessern. Diese Dokumentation beschreibt meinen Lernprozess, die Methodik, die Herausforderungen und die Ergebnisse der finalen Iteration.

\paragraph{Lernprozess und Trainingsiterationen}

\subparagraph{Erste Iteration: Erkundung und Überaugmentation}

In der ersten Iteration testete ich das YOLOv11n-Modell mit einem stark augmentierten Datensatz (Rotation ±30°, Helligkeit ±50\%, Rauschen 10\%) in Google Colab. Der Trainingsbefehl lautete:

bashKopieren\verb|!yolo train model=yolo11n.pt data=/content/datasets/dataset/data.yaml epochs=50 imgsz=640 batch=16 device=0|Die Ergebnisse waren unbefriedigend (mAP@0.5 ca. 0.5), da die übermäßige Augmentation zu einem Überlernen künstlicher Muster führte, insbesondere bei der Klasse "Knoten."

\subparagraph{Zweite Iteration: Iterative Erweiterung und Überanpassung}

In der zweiten Iteration kombinierte ich einen unveränderten Datensatz mit zwei augmentierten Datensätzen und trainierte iterativ mit je 50 Epochen (insgesamt 150 Epochen). Der Befehl für die erste Phase war:

bashKopieren\verb|!yolo train model=yolo11n.pt data=/content/datasets/unverändert/data.yaml epochs=50 imgsz=640 batch=16 device=0 augment=True|Augmentationen umfassten horizontale Spiegelung, Rotation (±15°) und Helligkeit (±20\%). Die hohe Epochenzahl führte zu Überanpassung (Trainings-mAP 0.9, Validierungs-mAP 0.6), und ich erkannte Fehler in den Annotationen, die eine erneute Überarbeitung notwendig machten.

\subparagraph{Dritte Iteration: Kombination von Datensätzen und Finales Training}

In der dritten Iteration optimierte ich den Ansatz, indem ich einen unveränderten Datensatz mit einem augmentierten Datensatz kombinierte, um Robustheit und Generalisierung zu verbessern.

DatensatzkombinationDer unveränderte Datensatz umfasste 250 Bilder (ca. 80 pro Klasse), annotiert in Roboflow. Der augmentierte Datensatz wurde gemäß Empfehlungen für den Einsatz im Innenraum mit variierenden Licht- und Bodenbedingungen erstellt. Die Augmentationseinstellungen waren:

\begin{itemize} 
    \item \textbf{Helligkeit}: ±20\%, um Lichtvariationen abzubilden. 
    \item \textbf{Sättigung}: ±10\%, für Farbänderungen. 
    \item \textbf{Farbton}: ±5\%, für leichte Farbverschiebungen. 
    \item \textbf{Rauschen}: 5\% Gaußsches Rauschen, für Sensorrauschen. 
    \item \textbf{Unschärfe}: 3-Pixel-Gaußsche Unschärfe, für Bewegungseffekte. 
\end{itemize}
Pro Originalbild wurde ein augmentiertes Bild generiert, was den kombinierten Datensatz auf etwa 500 Bilder brachte. Die Kombination erfolgte in Google Colab wie folgt:

\begin{enumerate} 
    \item Herunterladen der Datensätze: pythonKopieren\verb|from roboflow import Roboflow rf = Roboflow(api_key="MEIN_API_KEY") project = rf.workspace("MEIN_WORKSPACE").project("MEIN_PROJEKT") dataset1 = project.version(1).download("yolov11") \textit{# Unverändert} dataset2 = project.version(2).download("yolov11") \textit{# Augmentiert}|  
    \item Zusammenführen der train-Daten: bashKopieren\verb|!mkdir /content/datasets/combined_dataset !mkdir -p /content/datasets/combined_dataset/train/images !mkdir -p /content/datasets/combined_dataset/train/labels !mkdir -p /content/datasets/combined_dataset/valid/images !mkdir -p /content/datasets/combined_dataset/valid/labels !mkdir -p /content/datasets/combined_dataset/test/images !mkdir -p /content/datasets/combined_dataset/test/labels !cp /content/datasets/MEIN_PROJEKT-1/train/images/* /content/datasets/combined_dataset/train/images/ !cp /content/datasets/MEIN_PROJEKT-1/train/labels/* /content/datasets/combined_dataset/train/labels/ !cp /content/datasets/MEIN_PROJEKT-2/train/images/* /content/datasets/combined_dataset/train/images/ !cp /content/datasets/MEIN_PROJEKT-2/train/labels/* /content/datasets/combined_dataset/train/labels/ !cp /content/datasets/MEIN_PROJEKT-1/valid/images/* /content/datasets/combined_dataset/valid/images/ !cp /content/datasets/MEIN_PROJEKT-1/valid/labels/* /content/datasets/combined_dataset/valid/labels/ !cp /content/datasets/MEIN_PROJEKT-1/test/images/* /content/datasets/combined_dataset/test/images/ !cp /content/datasets/MEIN_PROJEKT-1/test/labels/* /content/datasets/combined_dataset/test/labels/|  
    \item Erstellung der data.yaml-Datei: pythonKopieren\verb|import yaml data_yaml = {  'train': '/content/datasets/combined_dataset/train/images',  'val': '/content/datasets/combined_dataset/valid/images',  'test': '/content/datasets/combined_dataset/test/images',  'nc': 3,  'names': ['Kegel', 'Knoten', 'Hindernis'] } with open('/content/datasets/combined_dataset/data.yaml', 'w') as f:  yaml.dump(data_yaml, f, default_flow_style=False)|  
\end{enumerate}
Die Validierungs- und Testdaten stammten ausschließlich aus dem unveränderten Datensatz, um Datenlecks zu vermeiden. Die Konsistenz wurde durch Vergleich von Bild- und Label-Dateien überprüft.

Training und ErgebnisseDas Modell wurde mit folgendem Befehl trainiert:

bashKopieren\verb|!yolo train model=yolo11n.pt data=/content/datasets/combined_dataset/data.yaml epochs=100 imgsz=640 batch=16 device=0 augment=True|Die Validierungsergebnisse lauten:

\begin{itemize} 
    \item \textbf{Modellarchitektur}: YOLOv11n (fused): 238 Schichten, 2.582.737 Parameter, 6.3 GFLOPs. 
    \item \textbf{Gesamtleistung}: 
    \begin{itemize} 
        \item Bilder: 22 
        \item Instanzen: 34 
        \item Präzision (P): 0.734 
        \item Rückruf (R): 0.760 
        \item mAP@0.5: 0.800 
        \item mAP@0.5:0.95: 0.595 
    \end{itemize}  
    \item \textbf{Klassenleistung}: 
    \begin{itemize} 
        \item \textbf{Kegel} (9 Bilder, 9 Instanzen): 
        \begin{itemize} 
            \item Präzision: 0.796 
            \item Rückruf: 1.0 
            \item mAP@0.5: 0.955 
            \item mAP@0.5:0.95: 0.824 
        \end{itemize}  
        \item \textbf{Knoten} (16 Bilder, 17 Instanzen): 
        \begin{itemize} 
            \item Präzision: 0.759 
            \item Rückruf: 0.529 
            \item mAP@0.5: 0.611 
            \item mAP@0.5:0.95: 0.299 
        \end{itemize}  
        \item \textbf{Hindernis} (8 Bilder, 8 Instanzen): 
        \begin{itemize} 
            \item Präzision: 0.648 
            \item Rückruf: 0.750 
            \item mAP@0.5: 0.833 
            \item mAP@0.5:0.95: 0.662 
        \end{itemize}  
    \end{itemize}  
    \item \textbf{Geschwindigkeit} (pro Bild): 
    \begin{itemize} 
        \item Vorverarbeitung: 0.1 ms 
        \item Inferenz: 1.7 ms 
        \item Nachverarbeitung: 1.0 ms 
        \item Gesamt: \~2.8 ms (ca. 357 FPS auf GPU) 
    \end{itemize}  
\end{itemize}
Die Ergebnisse zeigen eine starke Leistung bei "Kegel" und "Hindernis," während "Knoten" eine Schwäche darstellt, möglicherweise aufgrund unzureichender Daten oder Annotationen.

\paragraph{Reflexion des Lernprozesses}

Mein Lernprozess war iterativ geprägt. Die erste Iteration zeigte die Risiken von Überaugmentation, die zweite die Folgen von Überanpassung und fehlerhaften Annotationen. In der dritten Iteration integrierte ich diese Erkenntnisse, indem ich einen ausgewogenen Datensatz erstellte und die Epochenzahl optimierte. Google Colab und Roboflow waren entscheidend für den Erfolg.

\paragraph{Fazit und Ausblick}

Das Modell erreicht eine mAP@0.5 von 0.8, ist jedoch für den Raspberry Pi 5 noch zu optimieren (Bildgröße auf 416x416 anpassen). Ich plane, den Datensatz mit weiteren "Knoten"-Bildern zu erweitern. Diese Dokumentation dient als Grundlage für zukünftige Optimierungen.

 







\subsubsection{YOLOv11 Model konvertieren}
\label{convert-yolo}

Das .pt File des YOLO Models kann bereits eingelesen und verwendet werden. Es gibt Methoden, um diese Datei zu transformieren in ein anderes Format, damit auf embedded Systems und somit auch auf Raspberry Pis schneller die Bilderkennung durchgeführt werden kann.

Drei Möglichkeiten wurden betrachtet und verglichen:
TODO add sources for each \& akronyum
\begin{enumerate}
    \item Caffe2: In der Vergangenheit sehr geeignet, heutzutages mit PyTorch zusammengefügt und existiert nicht mehr auf diese Weise.
    \item NCNN: Sehr gute Performance für ARM Geräte (u.a. Raspberry Pi), sehr wenig Abhängigkeiten, einfache Transformation.
    \item ONXX: Geeignet für  Cross-Plattform Fälle, gute Austauschbarkeit des Models, flexibel, viele Abhängigkeiten.
\end{enumerate}

Es wird NCNN gewählt, da dies perfekt für den Use Case mit einem Raspberry Pi passt. Das erstellt .pt File kann mit der ultralytics Bibliothek transformiert werden:

\begin{verbatim}
yolo export model=best.pt format=ncnn
\end{verbatim}

Das Resultat ist ein Ordner mit zwei Dateien drin, mit allen nötigen Informationen, aus dem .pt File. Dieser Ordner kann im Python Code geladen werden, um die Bilderkennung durchzuführen. Mit der Transformation wurde das Risiko 3: 4 Minuten reichen nicht für einen Durchgang, weiter gemindert.

TODO: ADD NUMBERS, BEFORE (~1s) vs AFTER

\subsubsection{Modelresultate auswerten}
\label{model-results}

Um die Modelresultate auszuwerten wurde das Model in den Ordner der Navigation kopiert und es wurde eine ObjectDetector Klasse und ein Object Enum erstellt.

TODO UML

Diese ObjectDetector Klasse erstellt das Model aufgrund des NCNN Ordners bei der Instanzierung eines Objektes. Diese Objekt wird aufgerufen, wenn ein Nachbarsknoten geprüft werden soll und führt einen Prozess von drei Schritten durch:

\begin{enumerate}
    \item Inference (Objekterkennung)
    \item Resultat parsen
    \item Nächstes Objekte vor dem Roboter finden.
\end{enumerate}

Im Teil der Inference wird ein Bild an das Model gegeben. Dabei sollen nur die erkannten Objekte zurückgegeben werden, die mit einer definierten prozentualen Gewissheit erkannt wurden. Diese Gewissheit ist als Konstante definiert auf TODO WERT \& WIESO. \& falls Risiko damit vermindert (Falls erhoeht), Risiko 7

Diese Resultate werden dann geparsed, sodass alle erkannten Objekte mir ihrer ID, mit der Confidence, mit der sie erkannt wurden, und mit ihrem Standort auf dem Bild zurückgegeben werden.

Aus dieser Liste werden nun alle Objekte betrachtet, die sich auf der Mittellinie des Bildes befinden. Somit kann sichergestellt werden, dass nicht aus Versehen Objekte, die sich nicht auf der Fahrbahn, die betrachtet wird, befinden gespeichert werden und auch Risiko 7 (Objekte werden fälschlicherweise erkannt) kann so mitigiert werden. Die Mitellinie wird berechnet aus der Breite des Bildes. Diese Objekte werden sortiert nach ihren Koordinaten auf dem Bild. Das erste Element in dieser List, ist das nächste Objekt zum Roboter. Die Id des Objektes, die vom Model zurückgegben wird, korrespondiert mit der ID des erstellten Enums, damit das erkannte Objekt als Enum zurückgegeben wird.

In folgenden Fällen wird nicht einfach das nächste Objekt zurückgegeben:

\begin{itemize}
    \item Falls das nächste Objekt eine Barriere und das zweitnächste eine Pylone ist. In diesem Fall wird zurückgegeben, dass eine Pylone das nächste Objekt ist, da dieser Knoten sowieso nicht befahrbar ist und aus dem internen Graph entfernt werden soll.
    \item Falls kein Objekt erkannt wurde, wird ein Knoten zurückgegeben. Es ist weniger wahrscheinlich, dass eine Barriere oder ein Pylon verpasst werden, als dass ein Knoten nicht erkannt wird. Somit wird Risiko 2 (Knoten werden nicht erkannt) behandelt. Der Ultraschall kann trotzdem Objekte noch erkennen, falls ein unterwartetes Objekt auftritt. Es ist ein kleineres Problem ein Objekt zu verpassen, als sich eines einzubilden und fälschlicherweise Strecken zu entfernen. Durch den Ultraschall wird Risiko 12 und 1 (Objekte werden nicht erkannt) mitigiert.
\end{itemize}


Wie auf die einzelnen Objekte reagiert wird, ist in folgender Aufzählung beschrieben und ist gleich, wie in \acrshort{pren1} geplant.

\textbf{Pylonen erkennen}

Wird eine Pylone erkannt, wird der Knoten, der gerade geprüft wurde, inklusive alle Strecken dahin, aus dem internen Graphen entfernt.

\textbf{Knoten erkennen}

Wenn ein Knoten erkannt wird, dann geschieht nichts. Es wird interpretiert, dass sich kein Objekt auf diesem Weg befindet und die Strecke normal befahrbar ist.

\textbf{Barrieren erkennen}

Wir auf dem Bild eine Barriere erkannt, wird dies im internen Graphen gespeichert, indem die jeweilige Linie höher gewichtet wird, da es länger dauern wird diese zu überqueren.

\textbf{Entfernte Linien erkennen}

TODO LUKAS

\newpage
%%%%%%%%%%%%%%%%%Epic 11%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Zieleingabe}

\subsubsection{Peripherie}
\label{zieleingabe}

Geht sehr schnell: 1 Minute aufstellen gemindert Risiko